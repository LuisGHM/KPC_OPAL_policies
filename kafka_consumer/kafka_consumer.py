import os
import time
import json
import logging
import requests
import threading
from datetime import datetime
from typing import List, Dict, Any, Optional
from kafka import KafkaConsumer
from kafka.errors import NoBrokersAvailable
from requests.exceptions import ConnectionError, Timeout
from cachetools import TTLCache

# Configura√ß√£o de logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - [%(process)d] - %(message)s',
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger("kafka_consumer")

# Carregando vari√°veis de ambiente
KAFKA_BROKER = os.getenv("KAFKA_BROKER")
TOPICS = os.getenv("TOPICS", "").split(",")
OPAL_SERVER_URL = os.getenv("OPAL_SERVER_URL")
OPAL_DATASOURCE_TOKEN = os.getenv("OPAL_DATASOURCE_TOKEN")
OPA_BASE_URL = os.getenv("OPA_BASE_URL", "http://opal_client:8181")
MAX_RETRIES = int(os.getenv("MAX_RETRIES", "5"))
RETRY_DELAY_SECONDS = int(os.getenv("RETRY_DELAY_SECONDS", "5"))
CACHE_MAX_SIZE = int(os.getenv("CACHE_MAX_SIZE", "1000"))
CACHE_TTL_SECONDS = int(os.getenv("CACHE_TTL_SECONDS", "300"))  # 5 minutos

# Caches em mem√≥ria
# Para entidades OPA
entity_cache = TTLCache(maxsize=CACHE_MAX_SIZE, ttl=CACHE_TTL_SECONDS)
# Para relacionamento device -> roles
device_roles_cache = TTLCache(maxsize=CACHE_MAX_SIZE, ttl=CACHE_TTL_SECONDS)
# Para armazenar estado completo de cada device vindo de Debezium
devices_cache: Dict[int, Dict[str, Any]] = {}
# Cache para relacionamento employee -> roles
employee_roles_cache = TTLCache(maxsize=CACHE_MAX_SIZE, ttl=CACHE_TTL_SECONDS)
# Para armazenar estado completo de cada employee vindo de Debezium
employees_cache: Dict[int, Dict[str, Any]] = {}


def exponential_backoff(retry_count: int, base_delay: int = RETRY_DELAY_SECONDS) -> int:
    """
    Calcula o tempo de espera usando backoff exponencial
    """
    return min(base_delay * (2 ** retry_count), 60)


def wait_for_kafka(broker: str, retries: int = 10) -> None:
    """
    Tenta se conectar ao broker Kafka usando backoff exponencial
    """
    for i in range(retries):
        try:
            consumer = KafkaConsumer(bootstrap_servers=broker)
            consumer.close()
            logger.info("‚úÖ Conectado ao Kafka com sucesso!")
            return
        except NoBrokersAvailable:
            delay = exponential_backoff(i)
            logger.warning(f"‚ö†Ô∏è Broker {broker} n√£o dispon√≠vel. Tentando novamente em {delay}s... (tentativa {i+1}/{retries})")
            time.sleep(delay)
    raise Exception("‚ùå Kafka n√£o est√° dispon√≠vel ap√≥s v√°rias tentativas.")


def safe_deserializer(x: bytes) -> Optional[Dict[str, Any]]:
    if x is None:
        return None
    try:
        return json.loads(x.decode('utf-8'))
    except json.JSONDecodeError as e:
        logger.error(f"‚ùå Erro ao desserializar mensagem: {e}")
        return None
    except Exception as e:
        logger.error(f"‚ùå Erro inesperado ao processar mensagem: {e}")
        return None


def determine_entity_type(topic: str) -> str:
    if "Devices_devices" in topic and "Devices_devices_roles" not in topic:
        return "devices"
    elif "Devices_devices_roles" in topic:
        return "devices_roles"
    elif "Employees_employees" in topic and "Employees_employees_roles" not in topic:
        return "employees"
    elif "Employees_employees_roles" in topic:
        return "employees_roles"
    return "unknown"

def process_employee_role_event(event: Dict[str, Any]) -> None:
    if not validate_event(event):
        return
    payload = event["payload"]
    op_type = payload.get("op")
    if op_type in ("c", "u"):
        after = payload["after"]
        emp_id = after.get("employees_id")
        role_id = after.get("role_id")
        if not emp_id or not role_id:
            logger.warning(f"‚ö†Ô∏è Evento employees_roles sem employees_id ou role_id v√°lidos: {after}")
            return
        employee_roles_cache.setdefault(emp_id, set()).add(role_id)
        logger.info(f"‚úÖ Adicionada role {role_id} ao employee {emp_id} no cache")
        threading.Timer(1.0, update_employee_in_opal, args=[emp_id]).start()
    elif op_type == "d":
        before = payload["before"]
        emp_id = before.get("employees_id")
        role_id = before.get("role_id")
        if emp_id in employee_roles_cache and role_id in employee_roles_cache[emp_id]:
            employee_roles_cache[emp_id].remove(role_id)
            logger.info(f"‚úÖ Removida role {role_id} do employee {emp_id} no cache")
            threading.Timer(1.0, update_employee_in_opal, args=[emp_id]).start()

def fetch_employee_roles(emp_id: int) -> List[int]:
    return list(employee_roles_cache.get(emp_id, []))

def update_employee_in_opal(emp_id: int) -> None:
    try:
        emp_data = employees_cache.get(emp_id)
        if not emp_data:
            logger.warning(f"‚ö†Ô∏è Estado de employee {emp_id} n√£o est√° em cache, pulando update")
            return

        roles = fetch_employee_roles(emp_id)
        data = emp_data.copy()
        data["roles"] = roles
        logger.info(f"‚úÖ Preparando patch para employee {emp_id} com roles: {roles}")

        # AQUI VOC√ä ESCOLHE ADD vs REPLACE
        if does_entity_exist_in_opa(emp_id, "employees"):
            op_type = "replace"
        else:
            op_type = "add"

        patch = [{
            "op": op_type,
            "path": f"/{emp_id}",
            "value": data
        }]

        notify_opal(
            {"patch": patch, "entity_type": "employees"},
            reason=f"OPAL Patch: {op_type.upper()} employee {emp_id} roles - Automatic update"
        )
    except Exception as e:
        logger.error(f"‚ùå Erro ao atualizar employee {emp_id} no OPAL: {e}")

def process_device_role_event(event: Dict[str, Any]) -> None:
    if not validate_event(event):
        return
    payload = event.get("payload", {})
    op_type = payload.get("op")
    if op_type in ("c", "u"):
        after = payload.get("after", {})
        device_id = after.get("devices_id")
        role_id = after.get("role_id")
        if not device_id or not role_id:
            logger.warning(f"‚ö†Ô∏è Evento devices_roles sem device_id ou role_id v√°lidos: {after}")
            return
        device_roles_cache.setdefault(device_id, set()).add(role_id)
        logger.info(f"‚úÖ Adicionada role {role_id} ao device {device_id} no cache")
        threading.Timer(1.0, update_device_in_opal, args=[device_id]).start()
    elif op_type == "d":
        before = payload.get("before", {})
        device_id = before.get("devices_id")
        role_id = before.get("role_id")
        if not device_id or not role_id:
            logger.warning(f"‚ö†Ô∏è Evento DELETE sem device_id ou role_id v√°lidos: {before}")
            return
        if device_id in device_roles_cache and role_id in device_roles_cache[device_id]:
            device_roles_cache[device_id].remove(role_id)
            logger.info(f"‚úÖ Removida role {role_id} do device {device_id} no cache")
            threading.Timer(1.0, update_device_in_opal, args=[device_id]).start()


def update_device_in_opal(device_id: int) -> None:
    try:
        device_data = devices_cache.get(device_id)
        if not device_data:
            logger.warning(f"‚ö†Ô∏è Estado de device {device_id} n√£o est√° em cache, pulando update")
            return
        roles = list(device_roles_cache.get(device_id, []))
        data = device_data.copy()
        data["roles"] = roles
        logger.info(f"‚úÖ Atualizando device {device_id} no OPAL com roles: {roles}")
        patch = [{
            "op": "replace",
            "path": f"/{device_id}",
            "value": data
        }]
        patch_data = {"patch": patch, "entity_type": "devices"}
        reason = f"OPAL Patch: UPDATE device {device_id} roles - Automatic update"
        notify_opal(patch_data, reason=reason)
    except Exception as e:
        logger.error(f"‚ùå Erro ao atualizar device {device_id} no OPAL: {e}")


def fetch_device_roles(device_id: int) -> List[int]:
    return list(device_roles_cache.get(device_id, []))


def does_entity_exist_in_opa(entity_id: str, entity_type: str) -> bool:
    cache_key = f"{entity_type}:{entity_id}"
    if cache_key in entity_cache:
        return entity_cache[cache_key]
    url = f"{OPA_BASE_URL}/v1/data/{entity_type}/{entity_id}"
    for retry in range(MAX_RETRIES):
        try:
            resp = requests.get(url, timeout=5)
            if resp.status_code == 200:
                entity_cache[cache_key] = True
                return True
            elif resp.status_code == 404:
                entity_cache[cache_key] = False
                return False
            if retry < MAX_RETRIES - 1:
                time.sleep(exponential_backoff(retry))
                continue
            return False
        except (ConnectionError, Timeout) as e:
            if retry < MAX_RETRIES - 1:
                time.sleep(exponential_backoff(retry))
            else:
                return False
        except Exception:
            return False
    return False


def validate_event(event: Dict[str, Any]) -> bool:
    if not isinstance(event, dict) or 'payload' not in event:
        return False
    payload = event['payload']
    if not isinstance(payload, dict) or payload.get('op') not in ('c','u','d'):
        return False
    if payload['op'] in ('c','u') and 'after' not in payload:
        return False
    if payload['op'] == 'd' and 'before' not in payload:
        return False
    return True


def build_patch_from_event(event: Dict[str, Any], topic: str) -> Optional[Dict[str, Any]]:
    if not validate_event(event):
        return None

    payload = event["payload"]
    op_type = payload["op"]
    after = payload.get("after", {}) or {}
    before = payload.get("before", {}) or {}
    primary_key = "id"
    entity_type = determine_entity_type(topic)

    # processa apenas roles
    if entity_type == "devices_roles":
        process_device_role_event(event)
        return None
    if entity_type == "employees_roles":
        process_employee_role_event(event)
        return None

    # injeta roles em devices
    if entity_type == "devices" and op_type in ("c", "u"):
        device_id = after.get(primary_key)
        if device_id is not None:
            devices_cache[device_id] = after.copy()
            roles = fetch_device_roles(device_id)
            after["roles"] = roles
            logger.info(f"‚úÖ Adicionadas {len(roles)} roles ao device {device_id}")

    # injeta roles em employees
    if entity_type == "employees" and op_type in ("c", "u"):
        emp_id = after.get(primary_key)
        if emp_id is not None:
            employees_cache[emp_id] = after.copy()
            roles = fetch_employee_roles(emp_id)
            after["roles"] = roles
            logger.info(f"‚úÖ Adicionadas {len(roles)} roles ao employee {emp_id}")

    # monta a lista de opera√ß√µes PATCH/ADD/REMOVE
    patch_list = None
    if op_type == "c" and after.get(primary_key) is not None:
        entity_id = after[primary_key]
        patch_list = [{"op": "add", "path": f"/{entity_id}", "value": after}]
    elif op_type == "u" and after.get(primary_key) is not None:
        entity_id = after[primary_key]
        if does_entity_exist_in_opa(entity_id, entity_type):
            patch_list = [{"op": "replace", "path": f"/{entity_id}", "value": after}]
        else:
            patch_list = [{"op": "add", "path": f"/{entity_id}", "value": after}]
    elif op_type == "d" and before.get(primary_key) is not None:
        entity_id = before[primary_key]
        cache_key = f"{entity_type}:{entity_id}"
        if cache_key in entity_cache:
            del entity_cache[cache_key]
        patch_list = [{"op": "remove", "path": f"/{entity_id}"}]

    if patch_list:
        return {"patch": patch_list, "entity_type": entity_type}
    return None

def notify_opal(patch_data: Dict[str, Any], reason: str = "Debezium event -> incremental patch") -> bool:
    if not patch_data or 'patch' not in patch_data:
        return False
    payload = {
        'id': f"update-{datetime.now().isoformat()}",
        'entries': [{
            'url': '',
            'config': {},
            'topics': ['policy_data'],
            'dst_path': f"/{patch_data['entity_type']}",
            'save_method': 'PATCH',
            'data': patch_data['patch']
        }],
        'reason': reason,
        'callback': {'callbacks': []}
    }
    headers = {
        'Authorization': f"Bearer {OPAL_DATASOURCE_TOKEN}",
        'Content-Type': 'application/json'
    }
    for retry in range(MAX_RETRIES):
        try:
            resp = requests.post(OPAL_SERVER_URL, headers=headers, json=payload, timeout=10)
            if resp.status_code in (200,201,202,204):
                logger.info(f"‚úÖ Servidor OPAL notificado com sucesso para {patch_data['entity_type']}. Status: {resp.status_code}")
                return True
            if retry < MAX_RETRIES - 1:
                time.sleep(exponential_backoff(retry))
            else:
                logger.error(f"‚ùå Falha ao notificar OPAL ap√≥s {MAX_RETRIES} tentativas.")
                return False
        except (ConnectionError, Timeout):
            if retry < MAX_RETRIES - 1:
                time.sleep(exponential_backoff(retry))
            else:
                return False
        except Exception as e:
            logger.error(f"‚ùå Erro inesperado ao notificar OPAL: {e}")
            return False
    return False


def process_message(message) -> None:
    event = message.value
    if event is None:
        return
    topic = message.topic
    entity_type = determine_entity_type(topic)
    if entity_type == 'devices_roles':
        process_device_role_event(event)
        return
    patch_data = build_patch_from_event(event, topic)
    if patch_data:
        op = event['payload']['op']
        entity_id = (event['payload'].get('after') or event['payload'].get('before') or {}).get('id')
        operation_types = {'c':'CREATE','u':'UPDATE','d':'DELETE'}
        reason = f"OPAL Patch: {operation_types.get(op,'UNKNOWN')} {patch_data['entity_type']} id={entity_id}"
        notify_opal(patch_data, reason=reason)


def consume_kafka() -> None:
    stop_event = threading.Event()

    def signal_handler(sig, frame):
        stop_event.set()

    # monta lista de t√≥picos iniciais
    all_topics = TOPICS.copy() if isinstance(TOPICS, list) else TOPICS.split(",")

    # garante que os relacionamentos de roles ser√£o consumidos
    rel_topics = [
        "EventNotifier.public.Devices_devices_roles",
        "EventNotifier.public.Employees_employees_roles",
    ]
    for rel in rel_topics:
        if rel not in all_topics:
            all_topics.append(rel)

    consumer = KafkaConsumer(
        *all_topics,
        bootstrap_servers=KAFKA_BROKER,
        auto_offset_reset="earliest",
        enable_auto_commit=True,
        value_deserializer=safe_deserializer,
        group_id=os.getenv("KAFKA_GROUP_ID", "opal_consumer_group"),
        max_poll_interval_ms=300000,
        session_timeout_ms=30000,
        heartbeat_interval_ms=10000,
    )

    while not stop_event.is_set():
        messages = consumer.poll(timeout_ms=1000, max_records=10)
        for tp, msgs in messages.items():
            for message in msgs:
                process_message(message)
        consumer.commit()

    consumer.close(autocommit=True)

if __name__ == "__main__":
    logger.info("üöÄ Iniciando Kafka Consumer para OPAL")
    try:
        wait_for_kafka(KAFKA_BROKER)
        consume_kafka()
    except Exception as e:
        logger.critical(f"‚ùå Erro fatal: {e}", exc_info=True)
        exit(1)
